---
title: "Real Estate Predictive Modeling"
author: "Ben Weitzner"
date: "`r Sys.Date()`"
output:
  pdf_document:
    toc: true
  html_document:
    toc: true
    toc_float: true
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

# Libraries
```{r}
library(dplyr)
library(tidyverse)
library(zoo)
library(fredr)
library(forcats)
library(glmnet)
library(pROC)
library(ggplot2)
library(Matrix)
library(tidymodels)
library(randomForest)
library(rpart)
library(logistf)
library(ranger)
```

# Read in Real Estate Sales
```{r}
re <- read.csv("Real_Estate_Sales_2001-2023_GL_20251112.csv")

# Convert currency columns to numeric
re$Assessed.Value <- as.numeric(gsub("[$,]", "", re$Assessed.Value))
re$Sale.Amount    <- as.numeric(gsub("[$,]", "", re$Sale.Amount))

# Remove rows where Sale or Assessed Value == 0
re <- re %>% 
  filter(re$Assessed.Value != 0,
         re$Sale.Amount    != 0)

# Adjust assessment value to market value (CT assessment = 70% of market)
re$Market.Value <- re$Assessed.Value / 0.7

# Market Value Ratio to Sale amount
re$mv.ratio <- re$Sale.Amount / re$Market.Value

# Count above 1 to confirm that we have a viable number of above mv.ratio values to work with
sum(re$mv.ratio > 1)
```

# FRED API Setup and Pull Function
```{r}
#API key from Fred to call to
fredr_set_key("d18e4d190e7033ec5f7f3ae55e0f0f7c")

#function to grab data from Fred and add it to data set.
pull_and_join_quarterly <- function(series_ids) {
  
  df_final <- NULL
  
  for (id in series_ids) {
    
    # Pull monthly data between 2002 and 2023
    df_m <- fredr(
      series_id = id,
      observation_start = as.Date("2002-01-01"),
      observation_end   = as.Date("2023-01-01")
    )
    
    # NASDAQ has missing values for Januar 2002, so filling the data backwards to adjust
    df_m_filled <- df_m %>%
      arrange(date) %>%
      mutate(value = na.locf(value, fromLast = TRUE, na.rm = FALSE))
    
    # Converting monthly to Quarterly
    df_q <- df_m_filled %>%
      mutate(date_q = as.yearqtr(date)) %>%
      group_by(date_q) %>%
      summarise(!!id := last(value))

    #to allow the dataset loop to begin
    if (is.null(df_final)) {
      df_final <- df_q
    } else {
      df_final <- full_join(df_final, df_q, by = "date_q")
    }
  }
  
  return(df_final)
}
```

# Pull Variables and Merge with Real Estate Data
```{r}
#variables to pull
variables <- c( 
  "UNRATE", "MVMTD027MNFRBDAL","REITTMA","GDPC1","NASDAQCOM",
  "NASDAQNQEM","MEDCPIM158SFRBCLE","PCE","GDP"
)

#calling function
df <- pull_and_join_quarterly(variables)

#convert to dates
re$Date.Recorded <- as.Date(re$Date.Recorded, format = "%m/%d/%Y")

#quarter for merge with new data
re$date_q <- as.yearqtr(re$Date.Recorded)

#date of quarter end
re$date_q_end <- as.Date(re$date_q, frac = 1)

#merge
dre <- full_join(re, df, by = "date_q")
```

# Clean Assessor Remarks
```{r}
#Replacing Assessor Remarks with boolean
dre$Assessor.Remarks <- ifelse(
  trimws(dre$Assessor.Remarks) == "" | is.na(dre$Assessor.Remarks), NA,
  dre$Assessor.Remarks
)

dre$Remarks <- ifelse(is.na(dre$Assessor.Remarks), 0, 1)

#removing assessor remarks, as we do not care about the specific remarks
dre <- dre %>%
  dplyr::select(-c(
    Assessor.Remarks
  ))
```

# Fix Residential Type and Drop NA
```{r}
#Replacing null values in residential type with UNK(unknown)
dre$Residential.Type <- ifelse(
  trimws(dre$Residential.Type) == "" |
    is.na(dre$Residential.Type),
  NA,
  dre$Residential.Type
)

dre$Residential.Type <- ifelse(is.na(dre$Residential.Type), "UNK", dre$Residential.Type)

#Due to us having complete data only between Q1 2002, and Q4 2022, we need to make sure all data newer is deleted
dre <- dre[ as.numeric(substr(dre$date_q, 1, 4)) < 2023 , ]

#This converted rows outside our desired range to NA, so we deleted them
colSums(is.na(dre))
dre <- dre %>% drop_na()
```

# Convert to Factors and Select Final Columns
```{r}
dre <- dre %>%
  mutate(
    Property.Type    = as.factor(Property.Type),
    Residential.Type = as.factor(Residential.Type),
    Town             = as.factor(Town)
  )

#allows us to interpret factors compared to Unknown types
dre$Residential.Type <- relevel(dre$Residential.Type, ref = "UNK")

#Reduce Town to the top 10 towns + "Other" because it's also taking up too much RAM
dre$Town <- forcats::fct_lump(dre$Town, n = 10)

# Create prediction variable above_mv
dre <- dre %>%
  mutate(above_mv = ifelse(mv.ratio > 1, 1, 0))

dre$above_mv <- as.factor(dre$above_mv)
summary(dre$Remarks)

#Removes unneccessary columns
dre <- dre %>%
  dplyr::select(-c(
    Address,
    Non.Use.Code,
    OPM.remarks,
    Location,
    Serial.Number,
    Date.Recorded,
    mv.ratio,
    date_q,
    Sale.Amount,
    Sales.Ratio,
    Assessed.Value
  ))

#check amount of factors for each column for later
sapply(dre, function(x) if (is.factor(x)) nlevels(x) else NA)
```

# Reduce Dataset to 10% Stratified
```{r}
#computer refuses to run, so I randomly selected 10% of the rows.
drefull <- dre #storing full dataset incase it will be necessary later
dim(drefull)

set.seed(123)
summary(dre$above_mv)
neg <- 367932/1043350 #negative rows/total
pos <- 675418/1043350 #positive rows/total

totalrows <- 100000
nneg <- round(totalrows *neg)
npos <- round(totalrows*pos)

dre <- dre %>%
  group_by(above_mv) %>%
  sample_n(if_else(first(above_mv) == 0, nneg,npos)) %>%
  ungroup()
dim(dre)
```

# Train/Test Split
```{r}
# Train-test split (70/30)
set.seed(123)
sample_size <- floor(0.70 * nrow(dre))
train_indices <- sample(seq_len(nrow(dre)), size = sample_size)

train_data <- dre[train_indices, ]
test_data  <- dre[-train_indices, ]
summary(train_data)
dim(train_data)
```

# Random Forest Tuning Using Ranger
```{r}
#setting up random forest for use with Ranger
rf_model <- rand_forest(mtry=tune(),
                        trees = 200) %>%
  set_mode("classification") %>%
  set_engine("ranger", importance = "impurity")

#Creating Workflow
rf_wf <- workflow() %>% 
  add_model(rf_model) %>% 
  add_formula(above_mv ~ .)

folds<- vfold_cv(train_data, v=5) #splits into 5 different fold

#Tuning Forest
set.seed(123)
rf_tuned <- tune_grid(
  rf_wf,
  resamples = folds,
  grid = tibble(mtry = seq(from = 1, to = 13, by = 3)),
  metrics = metric_set(roc_auc)
)

#Looking at individual AUCs for later
rf_results <- rf_tuned %>%
  collect_metrics()

#make a QUALITY plot
ggplot(data=rf_results) + #not training
  geom_line(aes(x=mtry, y=mean))+
  labs(title = "AUC Performance Across mtry Values", x="mtry (Variables sampled per split)", y = "AUC")+
  theme_bw() +
  scale_x_continuous(breaks = seq(1, 13, by = 3))
```

# Final Random Forest Model
```{r}
#AUC was maximized under m = 4 to AUC = 0.817
#Automating ^
best_params <- select_best(rf_tuned, metric = "roc_auc")

#Building final forest with optimal information
set.seed(123)
finalforest <- ranger(
  above_mv ~ .,
  data = train_data,
  num.trees = 200,
  mtry = best_params$mtry,
  importance = "impurity",
  probability = TRUE
)

#pi_hat using test_data
pi_hat <- predict(finalforest, test_data)$predictions[, "1"]

rocCurve <- roc(response = test_data$above_mv,
                predictor = pi_hat,
                levels = c("0","1"))

plot(rocCurve, print.thres = TRUE, print.auc = TRUE)

#Automating optimal threshold
opt <- coords(rocCurve, "best", ret = "threshold")
opt <- as.numeric(opt)

#AUC = 0.818, which is alright, pi_star = 0.667, predict if threshold = 0.667, specificity(true negative) = 0.788, senstivity(true positive) = 0.710
rf_pred_class <- ifelse(pi_hat > opt, 1, 0)

table(Predicted = rf_pred_class, Actual = test_data$above_mv)
```

# Random Forest Variable Importance
```{r}
#Ranger requires a different vi - format was obtained from ChatGPT
vi <- data.frame(
  Variable = names(finalforest$variable.importance),
  Importance = as.numeric(finalforest$variable.importance)
)

#Vi graph for executive
ggplot(vi, aes(x = reorder(Variable, Importance), y = Importance)) +
  geom_col(fill = "steelblue") +
  coord_flip() +
  labs(x = "Variable", y = "Impurity Decrease",title = "Random Forest Variables"
  ) +
  theme_bw()
```

# GLM Model Comparison
```{r}
m0 <- glm(above_mv ~ Market.Value + MVMTD027MNFRBDAL + date_q_end,
          data = train_data, family = binomial(link = "logit"))
BIC(m0)

m1 <- glm(above_mv ~ Market.Value + MVMTD027MNFRBDAL + date_q_end + List.Year,
          data = train_data, family = binomial(link = "logit"))
BIC(m1)

m2 <- glm(above_mv ~ Market.Value + MVMTD027MNFRBDAL + date_q_end + List.Year + PCE,
          data = train_data, family = binomial(link = "logit"))
BIC(m2)

m3 <- glm(above_mv ~ Market.Value + MVMTD027MNFRBDAL + date_q_end + PCE + List.Year + GDP,
          data = train_data, family = binomial(link = "logit"))
BIC(m3)

m4 <- glm(above_mv ~ Market.Value + MVMTD027MNFRBDAL + date_q_end + PCE + List.Year + GDP + NASDAQCOM,
          data = train_data, family = binomial(link = "logit"))
BIC(m4)

m5 <- glm(above_mv ~ Market.Value + MVMTD027MNFRBDAL + date_q_end + PCE + List.Year + NASDAQCOM + GDP + Town,
          data = train_data, family = binomial(link = "logit"))
BIC(m5)

m6 <- glm(above_mv ~ Market.Value + MVMTD027MNFRBDAL + date_q_end + PCE + List.Year + NASDAQCOM + Town + GDP + REITTMA,
          data = train_data, family = binomial(link = "logit"))
BIC(m6)

m7 <- glm(above_mv ~ Market.Value + MVMTD027MNFRBDAL + date_q_end + PCE + List.Year + NASDAQCOM + Town + GDP + REITTMA + Remarks,
          data = train_data, family = binomial(link = "logit"))
BIC(m7)

m8 <- glm(above_mv ~ Market.Value + MVMTD027MNFRBDAL + date_q_end + PCE + List.Year + NASDAQCOM + Town + GDP + UNRATE + Remarks +REITTMA,
          data = train_data, family = binomial(link = "logit"))
BIC(m8)

m9 <- glm(above_mv ~ Market.Value + MVMTD027MNFRBDAL + date_q_end + PCE + List.Year + NASDAQCOM + Town + GDP + UNRATE + Remarks +REITTMA+Property.Type,
          data = train_data, family = binomial(link = "logit"))
BIC(m9)

m10 <- glm(above_mv ~ Market.Value + MVMTD027MNFRBDAL + date_q_end + PCE + List.Year + NASDAQCOM + Town + GDP + UNRATE + Remarks +REITTMA+Property.Type + Residential.Type,
          data = train_data, family = binomial(link = "logit"))
BIC(m10)

# BIC increased at m10, so I will keep m9 as my final model
beta <- coef(m9)
exp(beta)
```

# ROC Curve for Best Logistic Model
```{r}
glm_pred_prob <- predict(m9, newdata = test_data, type = "response")

glm_roc <- roc(response = test_data$above_mv,
               predictor = glm_pred_prob,
               levels = c("0","1"))

plot(glm_roc,main = "AUC Curve for Logistic Regression Model m9", print.auc = TRUE)
```

# LASSO and Ridge Setup
```{r}
# 1. Create x and y for glmnet
x.train <- model.matrix(above_mv ~ ., data = train_data)[,-1]  # remove intercept
y.train <- as.vector(as.numeric(as.character(train_data$above_mv)))  # vectorized

x.test  <- model.matrix(above_mv ~ ., data = test_data)[,-1]
y.test  <- as.vector(as.numeric(as.character(test_data$above_mv)))
```

# Penalized Logistic Regression (CV)
```{r}
# 2. Fit cross-validated LASSO and Ridge logistic regression
set.seed(123)
lr_lasso_cv <- cv.glmnet(x.train, y.train, family = "binomial", alpha = 1)

set.seed(123)
lr_ridge_cv <- cv.glmnet(x.train, y.train, family = "binomial", alpha = 0)

# 3. Plot cross-validation results
plot(lr_lasso_cv, main = "Cross Validation Curve Lasso Penalized Regression")
plot(lr_ridge_cv, main = "Cross Validation Curve Ridge Penalized Regression")

# 4. Extract best lambda
best_lasso_lambda <- lr_lasso_cv$lambda.min
best_ridge_lambda <- lr_ridge_cv$lambda.min

# 5. Coefficients from best models
lr_lasso_coefs <- coef(lr_lasso_cv, s = best_lasso_lambda)
lr_ridge_coefs <- coef(lr_ridge_cv, s = best_ridge_lambda)
```

# Final Penalized Fits and Predictions
```{r}
# 6. Fit final models at best lambda
final_lasso <- glmnet(x.train, y.train, family = "binomial", alpha = 1, lambda = best_lasso_lambda)
final_ridge <- glmnet(x.train, y.train, family = "binomial", alpha = 0, lambda = best_ridge_lambda)

# 7. Predict on test set
test.df.preds <- test_data %>%
  mutate(
    lasso_pred = predict(final_lasso, x.test, type = "response")[,1],
    ridge_pred = predict(final_ridge, x.test, type = "response")[,1]
  )
```

# LASSO & Ridge ROC Curves
```{r}
# 8. ROC curves
lasso_rocCurve <- roc(response = y.test, predictor = test.df.preds$lasso_pred)
ridge_rocCurve <- roc(response = y.test, predictor = test.df.preds$ridge_pred)

# 9. Prepare data for plotting
lasso_data <- data.frame(
  Model = "Lasso",
  Specificity = lasso_rocCurve$specificities,
  Sensitivity = lasso_rocCurve$sensitivities,
  AUC = as.numeric(lasso_rocCurve$auc)
)

ridge_data <- data.frame(
  Model = "Ridge",
  Specificity = ridge_rocCurve$specificities,
  Sensitivity = ridge_rocCurve$sensitivities,
  AUC = as.numeric(ridge_rocCurve$auc)
)

roc_data <- rbind(lasso_data, ridge_data)

# 10. Plot ROC curves
ggplot(roc_data) +
  geom_line(aes(x = 1 - Specificity, y = Sensitivity, color = Model)) +
  geom_text(data = roc_data %>% group_by(Model) %>% slice(1),
            aes(x = 0.75, y = c(0.65, 0.55), colour = Model,
                label = paste0(Model, " AUC = ", round(AUC, 3)))) +
  scale_colour_brewer(palette = "Paired") +
  labs(title = "Lasso and Ridge AUC Curves",x = "1 - Specificity", y = "Sensitivity", color = "Model") +
  theme_minimal()
```

# Summary Notes
```{r}
#As expected, the AUC of the RF (0.818) is the greatest of GLM (0.798), Lasso (0.799), and Ridge (0.791), 
#indicating it should be the predictive model.
```

```{r}
df_bar <- dre %>%
  group_by(Remarks) %>%
  summarise(MeanMarketValue = mean(Market.Value, na.rm = TRUE))

ggplot(df_bar, aes(x = factor(Remarks), y = MeanMarketValue)) +
  geom_col(fill = "grey") +
  labs(
    title = "Average Market Value by Assessor Remarks",
    x = "Remarks (0 = No, 1 = Yes)",
    y = "Average Market Value"
  ) +
  theme_bw()
```
```{r}
hdre <- dre
head(hdre)
hart <- hdre %>%
  filter(Town == "Hartford")

ggplot(hart, aes(x = above_mv)) +
  geom_bar(fill = "grey") +
  labs(
    title = "Distribution of Hartford Properties",
    x = "Above Market Value Count",
    y = "Count"
  ) +
  theme_bw()

glm_probs <- predict(m9, test_data, type = "response")
glm_pred <- ifelse(glm_probs > 0.5, 1, 0)
table(Predicted = glm_pred, Actual = test_data$above_mv)

lasso_class <- ifelse(test.df.preds$lasso_pred > 0.5, 1, 0)
ridge_class <- ifelse(test.df.preds$ridge_pred > 0.5, 1, 0)

table(Predicted = lasso_class, Actual = y.test)
table(Predicted = ridge_class, Actual = y.test)
```

